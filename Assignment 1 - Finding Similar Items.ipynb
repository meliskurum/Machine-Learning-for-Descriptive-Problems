{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004ecb93",
   "metadata": {},
   "source": [
    "## Task 1: Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041beed",
   "metadata": {},
   "source": [
    "Randomly select 1000 abstracts from the whole dataset. Find the similar items using pairwise Jaccard similarities, MinHash and LSH (vectorized versions) .\n",
    "\n",
    "   1. Compare the performance in time and the results for *k*-shingles = 3, 5 and 10, for the three methods and similarity thresholds *s*=0.1 and 0.2. Use 50 hashing functions. Comment your results. \n",
    "      \n",
    "   2. Compare the results obtained for MinHash and LSH for different similarity thresholds *s* = 0.5, 0.9 and 0.95  and 50, 100 and 200 hashing functions. Comment your results.\n",
    "   \n",
    "   3. For MinHashing using 100 hashing functions and s = 0.1 and 0.2, find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each s. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789842a",
   "metadata": {},
   "source": [
    "#    1. Compare the performance in time and the results for *k*-shingles = 3, 5 and 10, for the three methods and similarity thresholds *s*=0.1 and 0.2. Use 50 hashing functions. Comment your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a4309",
   "metadata": {},
   "source": [
    "Shingling is not useful while working with high number of files. Because it computes pairwise Jaccard similarities for every pair of docs and it takes lots of time. It took approximately 10 minutes.\n",
    "\n",
    "Minhashing converts large sets into smaller signatures, while preserving similarity. Hence it takes less time. It took around 10 seconds.\n",
    "\n",
    "LSH only focuses on pairs of signatures likely to be similar hence it shortens the time even more. It took less than 1 second.\n",
    "\n",
    "As k-shingles increases, the number of candidates decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11ed9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@proceedings{woah-2021-online,\n",
      "    title = \"Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)\",\n",
      "    editor = \"Mostafazadeh Davani, Aida  and\n",
      "      Kiela, Douwe  and\n",
      "      Lambert, Mathias  and\n",
      "      Vidgen, Bertie  and\n",
      "      Prabhakaran, Vinodkumar  and\n",
      "      Waseem, Zeerak\",\n",
      "    month = aug,\n",
      "    year = \"2021\",\n",
      "    address = \"Online\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"https://aclanthology.org/2021.woah-1.0\",\n",
      "}\n",
      "@inproceedings{singh-li-2021-exploiting,\n",
      "    title = \"Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers\",\n",
      "    author = \"Singh, Sumer  and\n",
      "      Li, Sheng\",\n",
      "    booktitle = \"Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)\",\n",
      "    month = aug,\n",
      "    year = \"2021\",\n",
      "    address = \"Online\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"https://aclanthology.org/2021.woah-1.1\",\n",
      "    doi = \"10.18653/v1/2021.woah-1.1\",\n",
      "    pages = \"1--5\",\n",
      "    abstract = \"Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.\",\n",
      "}\n",
      "@inproceedings{hahn-etal-2021-modeling,\n",
      "    title = \"Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces\",\n"
     ]
    }
   ],
   "source": [
    "# import texts\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "url = 'https://aclanthology.org/anthology+abstracts.bib.gz'\n",
    "with gzip.open(BytesIO(urlopen(url).read()), 'rb') as fb:\n",
    "    with open('test.bib', 'wb') as f:\n",
    "        f.write(fb.read())\n",
    "        \n",
    "file = open('test.bib')\n",
    "for n in range(30):\n",
    "    print(file.readline()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28dc7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding abstracts into a list\n",
    "\n",
    "list_of_abstracts = []\n",
    "key_word = '    abstract = \"'\n",
    "with open('anthology+abstracts.bib', encoding=\"utf8\") as bibtex_file:\n",
    "    for line in bibtex_file:\n",
    "        if line.startswith(key_word):\n",
    "            list_of_abstracts.append(line[len(key_word):-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9ff989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.',\n",
       " 'Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.',\n",
       " 'We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 abstracts\n",
    "list_of_abstracts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9248c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts: 30353\n"
     ]
    }
   ],
   "source": [
    "# number of abstracts\n",
    "print('Number of abstracts:', len(list_of_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1762c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{``}在汉语等其他有省略代词习惯的语言中,通常会删掉可从上下文信息推断出的代词。尽管以Transformer为代表的的神经机器翻译模型取得了巨大的成功,但这种省略现象依旧对神经机器翻译模型造成了很大的挑战。本文在Transformer基础上提出了一个融合零指代识别的翻译模型,并引入篇章上下文来丰富指代信息。具体地,该模型采用联合学习的框架,在翻译模型基础上,联合了一个分类任务,即判别句子中省略代词在句子所表示的成分,使得模型能够融合零指代信息辅助翻译。通过在中英对话数据集上的实验,验证了本文提出方法的有效性,与基准模型相比,翻译性能提升了1.48个BLEU值。{''} \n",
      "\n",
      "{``}机器译文自动评价对机器翻译的发展和应用起着重要的促进作用,它一般通过计算机器译文和人工参考译文的相似度来度量机器译文的质量。该文通过跨语种预训练语言模型XLM将源语言句子、机器译文和人工参考译文映射到相同的语义空间,结合分层注意力和内部注意力提取源语言句子与机器译文、机器译文与人工参考译文以及源语言句子与人工参考译文之间差异特征,并将其融入到基于Bi-LSTM神经译文自动评价方法中。在WMT{'}19译文自动评价数据集上的实验结果表明,融合XLM词语表示的神经机器译文自动评价方法显著提高了其与人工评价的相关性。{''} \n",
      "\n",
      "               empty    109\n",
      "    short(<200 char)     56\n",
      "           non latin    145\n",
      "               other  30043\n"
     ]
    }
   ],
   "source": [
    "# clean up the abstracts\n",
    "import re\n",
    "minimum = 200\n",
    "example = 0\n",
    "count = {'empty':0, \n",
    "         f'short(<{minimum} char)': 0,\n",
    "         'non latin': 0,\n",
    "         'other': 0}\n",
    "\n",
    "abstracts = []\n",
    "\n",
    "\n",
    "for a in list_of_abstracts:\n",
    "    if len(a) == 0:\n",
    "        count['empty'] +=1\n",
    "    elif len(re.findall('[a-zA-Z]', a)) < .2*len(a):\n",
    "        count['non latin'] +=1\n",
    "        if example < 2:\n",
    "            print(a, '\\n')\n",
    "            example +=1\n",
    "    elif len(a) < minimum:\n",
    "        count[f'short(<{minimum} char)'] += 1\n",
    "    else:\n",
    "              count['other'] +=1\n",
    "              abstracts.append(a)\n",
    "            \n",
    "for k,v in count.items():\n",
    "              print(f'{k:>20}', f'{v:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9274ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts after cleaning it up: 30043\n"
     ]
    }
   ],
   "source": [
    "print('Number of abstracts after cleaning it up:', len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9be2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts after selecting randomly: 1000\n"
     ]
    }
   ],
   "source": [
    "# randomly select 1000 abstracts from the whole cleaned dataset.\n",
    "import random\n",
    "abstracts = random.sample(abstracts, 1000)\n",
    "print('Number of abstracts after selecting randomly:', len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29141b",
   "metadata": {},
   "source": [
    "## Shingling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d165b40",
   "metadata": {},
   "source": [
    "### Shingling    k:3, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "from time import time\n",
    "\n",
    "def get_shingles(abstract, k=3):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1baf3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_vectors = []\n",
    "\n",
    "for file in abstracts: \n",
    "    sh = list(get_shingles(file, k=3))\n",
    "    shingles_vectors.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f8c6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2249034749034749"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity_score(x, y):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "jaccard_similarity_score(shingles_vectors[0], shingles_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24fb5725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 490516\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "\n",
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=3),get_shingles(pair[1], k=3))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce295f8",
   "metadata": {},
   "source": [
    "### Shingling k:3, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f21649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 290504\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = 0.2\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=3),get_shingles(pair[1], k=3))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e623e",
   "metadata": {},
   "source": [
    "### Shingling  k:5, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4da82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=5):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c598ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_vectors = []\n",
    "\n",
    "for file in abstracts: \n",
    "    sh = list(get_shingles(file, k=5))\n",
    "    shingles_vectors.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5719e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 3212\n",
      "Wall time: 10min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=5),get_shingles(pair[1], k=5))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd640e10",
   "metadata": {},
   "source": [
    "### Shingling k:5, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efdc3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 2\n",
      "Wall time: 11min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s = 0.2\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=5),get_shingles(pair[1], k=5))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82027b",
   "metadata": {},
   "source": [
    "### Shingling  k:10, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f135f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=10):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e20054",
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_vectors = []\n",
    "\n",
    "for file in abstracts: \n",
    "    sh = list(get_shingles(file, k=10))\n",
    "    shingles_vectors.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60a6ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1\n",
      "Wall time: 11min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "\n",
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=10),get_shingles(pair[1], k=10))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc92cfe",
   "metadata": {},
   "source": [
    "### Shingling k:10, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf341d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s = 0.2\n",
    "candidates = []\n",
    "\n",
    "for pair in itertools.combinations(abstracts,2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], k=10),get_shingles(pair[1], k=10))\n",
    "    \n",
    "    if js > s:\n",
    "        candidates.append(pair)\n",
    "        \n",
    "print(f'Number of candidates: {len(candidates)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef279f",
   "metadata": {},
   "source": [
    "## Minhashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b95117",
   "metadata": {},
   "source": [
    "### Minhashing  k:5, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b644328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=5):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a5c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use word-word co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) humour classification datasets using only 10{\\%} of known labels.\n",
      "number of shingles: 841\n"
     ]
    }
   ],
   "source": [
    "print(\"file:{}\".format(abstracts[0]))\n",
    "print(\"number of shingles: {}\".format(len(get_shingles(abstracts[0], k=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7f60e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c0f4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 [2608428423 2625020704  317044267 1726195090 4058616660 2280834150\n",
      "  642999764 1865178151 2934700102 3325181062 1874789733 1839672909\n",
      " 3498069159  408492336   90301516 1431956758 2729065828  136935333\n",
      " 1552452541 1794071519 1212668039    2943883 3633368108 2465656604\n",
      "  519014756 2112079809 2891052589 1563307481 4121614587  726915284\n",
      "  983608387 1863092456 2348804910 2495972336 1014574098  582140236\n",
      "  873127908  210182330 1908036527 4156881913 4082800064 1535204616\n",
      " 2398712886    2206787 1631686503 3482275748 1129636301 2108977863\n",
      "  421860581 2969324923]\n"
     ]
    }
   ],
   "source": [
    "print(len(A),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfd8ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random shingle: 1619480577\n",
      "its hash code by first hash function: 3933608767\n",
      "its hash code by second hash function: 3135531263\n"
     ]
    }
   ],
   "source": [
    "ShingleID = list(get_shingles(abstract, k=5))[0]\n",
    "\n",
    "print(\"random shingle: {}\".format(ShingleID))\n",
    "\n",
    "hashCode = ((A[0]*ShingleID + B[0]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by first hash function: {}\".format(hashCode))\n",
    "hashCode = ((A[1]*ShingleID + B[1]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by second hash function: {}\".format(hashCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ad0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive version of Minhash algorithm that computes a signature for a single file\n",
    "# all shingles from that file are given in 'shingles'\n",
    "\n",
    "def minhash(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = []\n",
    "    for i in range(nsig):  # number of hash functions == nsig\n",
    "        minHashCode = maxShingleID + 1\n",
    "        a = A[i]\n",
    "        b = B[i]\n",
    "        \n",
    "        for ShingleID in shingles:\n",
    "            hashCode = ((a*ShingleID + b) % nextPrime) % maxShingleID\n",
    "            if hashCode < minHashCode:\n",
    "                minHashCode = hashCode\n",
    "\n",
    "        signature.append(minHashCode)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfb9010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file signature: 50 , [5318889, 1649361, 768091, 2509403, 7600308, 13947042, 614913, 5650378, 8679378, 4864527, 5159907, 1240308, 5303800, 7347425, 3801393, 3426982, 9990741, 1245743, 3123, 6614762, 4818795, 5606449, 244496, 6248275, 1897902, 9215178, 2363666, 4405532, 5820443, 371064, 2914492, 5365917, 2761972, 2767852, 2236010, 2029690, 446360, 10616707, 2468366, 5894004, 20473406, 2785576, 5199159, 1245919, 4880741, 1561817, 7936019, 7816122, 1454999, 4022773]\n"
     ]
    }
   ],
   "source": [
    "shingles = get_shingles(abstract, k=5)\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime/2, size=(nsig,), dtype = numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime/2, size=(nsig,), dtype = numpy.int64)\n",
    "\n",
    "signature = minhash(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "print(\"file signature: {} , {}\".format(len(signature),signature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a06faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total signatures: 1000\n",
      "took 34.82668113708496 seconds\n"
     ]
    }
   ],
   "source": [
    "# compute Minhashes for all files using a slow naive code\n",
    "signatures = []\n",
    "t = time()\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "t1 = time()-t\n",
    "print(\"total signatures: {}\".format(len(signatures)))\n",
    "print(\"took {} seconds\".format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c25adec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast implementation of Minhash algorithm\n",
    "# computes all random hash functions for a shingle at once, using vector operations\n",
    "# also finds element-wise minimum of two vectors efficiently\n",
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e026d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow code took 35.158782720565796 seconds\n",
      "really fast code took 4.634079694747925 seconds\n",
      "speedup 7.587004332362543\n",
      "results are the same: True\n"
     ]
    }
   ],
   "source": [
    "# compare two versions of Minhash code\n",
    "shingles_all_files = []\n",
    "for fname in abstracts:\n",
    "    shingles_all_files.append(get_shingles(fname, k=5))\n",
    "\n",
    "t = time()\n",
    "signatures_all_files_1 = []\n",
    "for shingles in shingles_all_files:\n",
    "    signature = minhash(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures_all_files_1.append(signature)\n",
    "t1 = time()-t\n",
    "print(\"slow code took {} seconds\".format(t1))\n",
    "\n",
    "t = time()\n",
    "signatures_all_files_2 = []\n",
    "for shingles in shingles_all_files:\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures_all_files_2.append(signature)\n",
    "t2 = time()-t\n",
    "print(\"really fast code took {} seconds\".format(t2))\n",
    "\n",
    "print('speedup {}'.format(t1/t2))\n",
    "\n",
    "signatures_all_files_1 = numpy.array(signatures_all_files_1)\n",
    "signatures_all_files_2 = numpy.array(signatures_all_files_2)\n",
    "print(\"results are the same: {}\".format(numpy.allclose(signatures_all_files_1, signatures_all_files_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35b49320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.7157392501831055 seconds\n",
      "found 35318 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1989d",
   "metadata": {},
   "source": [
    "### Minhashing  k:5, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "868aa21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.965559482574463 seconds\n",
      "found 158 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c78c8",
   "metadata": {},
   "source": [
    "### Minhashing k:3, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec2ca5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=3):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbd4ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random shingle: 3230435331\n",
      "its hash code by first hash function: 712734687\n",
      "its hash code by second hash function: 2517094247\n"
     ]
    }
   ],
   "source": [
    "ShingleID = list(get_shingles(abstract, k=3))[0]\n",
    "\n",
    "print(\"random shingle: {}\".format(ShingleID))\n",
    "\n",
    "hashCode = ((A[0]*ShingleID + B[0]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by first hash function: {}\".format(hashCode))\n",
    "hashCode = ((A[1]*ShingleID + B[1]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by second hash function: {}\".format(hashCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fcba21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really fast code took 3.2175002098083496 seconds\n"
     ]
    }
   ],
   "source": [
    "shingles_all_files = []\n",
    "for fname in abstracts:\n",
    "    shingles_all_files.append(get_shingles(fname, k=3))\n",
    "\n",
    "t = time()\n",
    "signatures_all_files_2 = []\n",
    "for shingles in shingles_all_files:\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures_all_files_2.append(signature)\n",
    "t2 = time()-t\n",
    "print(\"really fast code took {} seconds\".format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dacf4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.830845832824707 seconds\n",
      "found 442783 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=3)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe4f06",
   "metadata": {},
   "source": [
    "### Minhashing  k:3, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24174c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.815581321716309 seconds\n",
      "found 162586 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=3)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d0444",
   "metadata": {},
   "source": [
    "### Minhashing k:10, s:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de1a2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=10):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f0d8900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random shingle: 1335128064\n",
      "its hash code by first hash function: 621178495\n",
      "its hash code by second hash function: 3207715175\n"
     ]
    }
   ],
   "source": [
    "ShingleID = list(get_shingles(abstract, k=10))[0]\n",
    "\n",
    "print(\"random shingle: {}\".format(ShingleID))\n",
    "\n",
    "hashCode = ((A[0]*ShingleID + B[0]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by first hash function: {}\".format(hashCode))\n",
    "hashCode = ((A[1]*ShingleID + B[1]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by second hash function: {}\".format(hashCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec35983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really fast code took 5.33053731918335 seconds\n"
     ]
    }
   ],
   "source": [
    "shingles_all_files = []\n",
    "for fname in abstracts:\n",
    "    shingles_all_files.append(get_shingles(fname, k=10))\n",
    "\n",
    "t = time()\n",
    "signatures_all_files_2 = []\n",
    "for shingles in shingles_all_files:\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures_all_files_2.append(signature)\n",
    "t2 = time()-t\n",
    "print(\"really fast code took {} seconds\".format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7e0d978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.058567047119141 seconds\n",
      "found 108 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=10)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eb11c",
   "metadata": {},
   "source": [
    "### Minhashing - k:10, s:0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4d826eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.747239351272583 seconds\n",
      "found 1 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=10)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae61aa7",
   "metadata": {},
   "source": [
    "### LSH - k:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f248005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0)\n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        numpy.sort(cand)\n",
    "                        candidates.add(tuple(cand))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c3e88bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.0995032787322998 seconds\n",
      "found 3638 candidates\n"
     ]
    }
   ],
   "source": [
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=3)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "#s = 0.95  # NO similarity threshold, why?\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3110a95",
   "metadata": {},
   "source": [
    "### LSH  k:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff299601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.05013537406921387 seconds\n",
      "found 4 candidates\n"
     ]
    }
   ],
   "source": [
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "#s = 0.95  # NO similarity threshold, why?\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22254a",
   "metadata": {},
   "source": [
    "### LSH  k:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f8a8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.03135251998901367 seconds\n",
      "found 0 candidates\n"
     ]
    }
   ],
   "source": [
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=10)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "#s = 0.95  # NO similarity threshold, why?\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff403fbd",
   "metadata": {},
   "source": [
    "# 2. Compare the results obtained for MinHash and LSH for different similarity thresholds s = 0.1, 0.2 and 0.25 and 50, 100 and 200 hashing functions. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3b3bf",
   "metadata": {},
   "source": [
    "As hashing fuctions increases, the number of candidates decreases. \n",
    "\n",
    "LSH is very fast, it takes less than 1 second. And by using LSH I found significantly less number of candidates especially comparing minhashing with s = 0.1 similarity threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f5105",
   "metadata": {},
   "source": [
    "### Minhash - s: 0.1, hashing:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7935bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=5):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79a0b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e91a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random shingle: 1619480577\n",
      "its hash code by first hash function: 4078442722\n",
      "its hash code by second hash function: 1490076031\n"
     ]
    }
   ],
   "source": [
    "ShingleID = list(get_shingles(abstract, k=5))[0]\n",
    "\n",
    "print(\"random shingle: {}\".format(ShingleID))\n",
    "\n",
    "hashCode = ((A[0]*ShingleID + B[0]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by first hash function: {}\".format(hashCode))\n",
    "hashCode = ((A[1]*ShingleID + B[1]) % nextPrime) % maxShingleID\n",
    "print(\"its hash code by second hash function: {}\".format(hashCode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daec18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really fast code took 4.582310914993286 seconds\n"
     ]
    }
   ],
   "source": [
    "shingles_all_files = []\n",
    "for fname in abstracts:\n",
    "    shingles_all_files.append(get_shingles(fname, k=5))\n",
    "\n",
    "t = time()\n",
    "signatures_all_files_2 = []\n",
    "for shingles in shingles_all_files:\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures_all_files_2.append(signature)\n",
    "t2 = time()-t\n",
    "print(\"really fast code took {} seconds\".format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2ff667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.636272430419922 seconds\n",
      "found 107813 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75c165",
   "metadata": {},
   "source": [
    "### Minhash - s:0.2, hashing:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b972f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.84703516960144 seconds\n",
      "found 1481 candidates\n"
     ]
    }
   ],
   "source": [
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531013b1",
   "metadata": {},
   "source": [
    "### Minhash - s:0.25, hashing:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "009c07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 5.9132020473480225 seconds\n",
      "found 41 candidates\n"
     ]
    }
   ],
   "source": [
    "s = 0.25  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade361fe",
   "metadata": {},
   "source": [
    "### Minhash - s:0.1, hashing:100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a133892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(abstract, k=5):\n",
    "    L = len(abstract)\n",
    "    shingles = set()          \n",
    "    for i in range(L-k+1):\n",
    "        shingles.add(binascii.crc32(abstract[i:i+k].encode('utf-8') ) )           \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27de522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 10\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8814a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.0353102684021 seconds\n",
      "found 36034 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335e4db",
   "metadata": {},
   "source": [
    "### Minhash - s:0.2, hashing: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51765b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.065131187438965 seconds\n",
      "found 34 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a30aa",
   "metadata": {},
   "source": [
    "### Minhash - s:0.25, hashing: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7153f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.364579439163208 seconds\n",
      "found 2 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.25  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981054d7",
   "metadata": {},
   "source": [
    "### Minhash - s:0.1, hashing: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ab85197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 20\n",
    "rows = 10\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bc4d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.3162922859191895 seconds\n",
      "found 17875 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e584bd",
   "metadata": {},
   "source": [
    "### Minhash - s:0.2, hashing: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a9e663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.480101585388184 seconds\n",
      "found 3 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032268f8",
   "metadata": {},
   "source": [
    "### Minhash - s:0.25, hashing: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9efc54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 6.232314109802246 seconds\n",
      "found 1 candidates\n"
     ]
    }
   ],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.25  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append((i,j))\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b25d49",
   "metadata": {},
   "source": [
    "### LSH 50 hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c0fbf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.05684399604797363 seconds\n",
      "found 6 candidates\n"
     ]
    }
   ],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 5\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "\n",
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "#s = 0.95  # NO similarity threshold, why?\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472af6fe",
   "metadata": {},
   "source": [
    "### LSH 100 hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3954e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.05050373077392578 seconds\n",
      "found 0 candidates\n"
     ]
    }
   ],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 10\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "\n",
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "#s = 0.95  # NO similarity threshold, why?\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad39f97",
   "metadata": {},
   "source": [
    "### LSH 200 hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e45ffb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 0.04929041862487793 seconds\n",
      "found 0 candidates\n"
     ]
    }
   ],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 20\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "\n",
    "# find candidates with LSH\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows), dtype = numpy.int64)  # now we need a vector of A parameters for each band\n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ), dtype = numpy.int64)\n",
    "signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "Nfiles = signatures.shape[1]  # number of different files\n",
    "t = time()\n",
    "candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb5a8f",
   "metadata": {},
   "source": [
    "## 3. For MinHashing using 100 hashing functions and s = 0.1 and 0.2, find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each s. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f62e9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 10\n",
    "nsig = bands*rows  # number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58c9e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get candidate pairs without Locality-Sensitive Hashing\n",
    "\n",
    "signatures = []  # signatures for all files\n",
    "\n",
    "for fname in abstracts:\n",
    "    shingles = get_shingles(fname, k=5)\n",
    "    signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "    signatures.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures)\n",
    "t = time()\n",
    "candidates = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates.append(Jsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aeb599cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting jaccard distances by using 1 - jaccard similarity\n",
    "\n",
    "distances = [1 - x for x in candidates]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
